{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d30d2b51c8c6fc8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Digesting Data from NGINX Access Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb031d247ee0892",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This notebook was created as an exercise in data manipulation and visualization in Python. The data used in this notebook consists of the last two months of searches on [GitHub Search](https://seart-ghs.si.usi.ch), as recorded in the NGINX access logs. The raw logs were parsed and enriched with geographical data, which was retrieved from [ipinfo](https://ipinfo.io). Our end-goal is to gain more insight into the usage of the aforementioned platform. Before we begin, we need to import all the necessary library utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "from geopandas import read_file\n",
    "from geopandas.datasets import get_path\n",
    "from matplotlib.pyplot import bar, figure, pie, show, subplot\n",
    "from pandas import DataFrame\n",
    "from pandas import concat, json_normalize, read_csv\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b2a96542cde",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "With that out of the way, we can now read the CSV file, and convert the string representation of the JSON data into Python dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3493fc45d52c1eb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = read_csv(\"access.csv\", header=None, names=[\"access\", \"city\"], converters={0: literal_eval, 1: literal_eval})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79bdef4cab05d55",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next, let's transform our two-column `DataFrame` into a single `DataFrame` with the JSON data expanded into columns. Let's start with the `city` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e4118623eb714e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mappings = {\n",
    "  \"name\": \"city\",\n",
    "  \"country_continent_name\": \"continent_name\",\n",
    "  \"country_continent_code\": \"continent_code\",\n",
    "  \"position_latitude\": \"latitude\",\n",
    "  \"position_longitude\": \"longitude\",\n",
    "}\n",
    "city = json_normalize(df.city, sep=\"_\").rename(columns=mappings)\n",
    "coordinate_mapper = lambda row: Point(row.longitude, row.latitude)\n",
    "city[\"coordinates\"] = city.apply(coordinate_mapper, axis=1)\n",
    "city.drop(columns=[\"latitude\", \"longitude\"], inplace=True)\n",
    "city = city[[\"city\", \"region\", \"country_name\", \"country_code\", \"continent_name\", \"continent_code\", \"coordinates\"]]\n",
    "city"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55499417b1d619",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's do the same for the `access` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ee56ea3d6f7f8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "access = json_normalize(df.access, sep=\"_\", max_level=0)\n",
    "access = access.join(json_normalize(access[\"user_agent\"], sep=\"_\"))\n",
    "access.drop(columns=[\"user_agent\"], inplace=True)\n",
    "access = access[[\"ip\", \"time\", \"query\", \"status\", \"size\", \"browser\", \"os\", \"device\", \"referer\"]]\n",
    "access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbbb3a21223a9ab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Merging the two frames together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f307d193859c1d57",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = DataFrame.join(access, city)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d98f0b25c12a2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can now initiate our analysis. Let's start off easy by just calculating the _total_ number of requests per day."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "date_only = lambda ts: ts.split(\"T\")[0]\n",
    "days_accessed = df.time.map(date_only)\n",
    "daily_visitors = days_accessed.value_counts().sort_index()\n",
    "daily_visitors.plot(figsize=(20, 10))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24fac13fac06de69",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Seems like the platform has been accessed every day. Let's break this 60-day period down into a statistical summary:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63f3e54f514bacca"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "daily_visitors.describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64db773e7c7db2c6",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the average number of daily searches is around 25, with the minimum being as low as only 2 searches in a day, and the maximum being more than 100 searches in a single day! Keep in mind that we are only looking at the number of requests made on a daily basis. This means that the number of unique visitors is likely to be lower. Over the last two months, the total number of unique IP addresses that have accessed the platform is:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5bbf2ce1cd10d98"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.ip.nunique()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b1ed0291ca5d10a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And counting unique visitors per day:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7916a932cce4ed25"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ip_access = df[['time', 'ip']]\n",
    "ip_access.loc[:, \"time\"] = ip_access.loc[:, \"time\"].map(date_only)\n",
    "ip_access_by_day = ip_access.groupby(\"time\").nunique()\n",
    "ip_access_by_day.plot(figsize=(20, 10))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac1984e358309543",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And on average:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "294ed47ae709f88"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ip_access_by_day.describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f14ac757b4a7cc71",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Having gathered insights on daily traffic, we can now move on calculating the distribution of traffic by continent and country."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "578954bc649eb755"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcee9e8c6a8bf8e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_top_n(data_frame: DataFrame, n: int = 10):\n",
    "  data_frame_counts = data_frame.value_counts()\n",
    "  data_frame_top = data_frame_counts.head(n - 1)\n",
    "  data_frame_other_count = data_frame_counts.tail(1 - n).sum()\n",
    "  data_frame_other = DataFrame(data={\"count\": [data_frame_other_count]}, index=[\"Other\"])\n",
    "  return concat([data_frame_top, data_frame_other])\n",
    "\n",
    "continent_top = get_top_n(df.continent_name, 5)\n",
    "country_top = get_top_n(df.country_name)\n",
    "\n",
    "autopct = \"%1.1f%%\"\n",
    "figure(figsize=(20, 10))\n",
    "subplot(1, 2, 1)\n",
    "pie(list(continent_top[\"count\"]), labels=continent_top.index, autopct=autopct)\n",
    "subplot(1, 2, 2)\n",
    "pie(list(country_top[\"count\"]), labels=country_top.index, autopct=autopct)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e9e988c8248242",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "While the distribution of traffic by continent is relatively balanced and simple to comprehend, the distribution of traffic by country appears to be more complex. More than a quarter of the traffic is widely distributed across a large number of countries. For this reason, plotting the data on a map might be more insightful. To do that, we must first load some geographical geometry data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9851894c491271c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_geometry = read_file(get_path(\"naturalearth_lowres\"))\n",
    "country_geometry = country_geometry[country_geometry.continent != \"Antarctica\"]\n",
    "country_geometry = country_geometry[country_geometry.pop_est > 0]\n",
    "country_geometry = country_geometry[[\"name\", \"geometry\"]]\n",
    "country_geometry = country_geometry.rename(columns={\"name\": \"country_name\"})\n",
    "country_geometry = country_geometry.set_index(\"country_name\")\n",
    "country_geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, let's join the geometry data of each individual country with the traffic data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee4c5ea2b80164bf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "country_count = df.country_name.value_counts()\n",
    "world_count = country_geometry.join(country_count)\n",
    "world_count"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aff62e87b71cfe20",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can plot the data on a map."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2a1a8d73872ef2f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa121b84fbcbae2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "world_count.plot(\n",
    "  figsize=(20, 10),\n",
    "  column=\"count\",\n",
    "  cmap=\"viridis\",\n",
    "  legend=True,\n",
    "  missing_kwds={\n",
    "    \"color\": \"lightgrey\",\n",
    "    \"label\": \"No Data\",\n",
    "    \"edgecolor\": \"red\",\n",
    "  },\n",
    "  legend_kwds={\n",
    "    \"label\": \"Traffic\",\n",
    "    \"shrink\": 0.65,\n",
    "  },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "From this we can not only see all the countries that the platform has been accessed from, but also the countries that it has not been accessed from. The US is of particular interest, as it has no recorded traffic. On the flipside, small amounts of trafic were recorded in Africa (Morocco and Egypt), as well as South America (Brazil and Chile). "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10b8f7cd47447b25"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Shifting our focus to a different angle, we can now analyze the distribution of traffic by browser, operating system, and device. To start, let's first isolate the relevant columns into a smaller `DataFrame`:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3470fe5b2bfdbe29"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "user_agent = df[[\"browser\", \"os\"]]\n",
    "user_agent"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d4dd5355578afcf",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that both `browser` and `os` contain versioning information, which may potentially lead to more numerous unique values. To reduce the values to their respective families, we can apply a simple transformation:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2123d2ad8405741"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "browsers = user_agent[[\"browser\"]].map(lambda x: x.rsplit(' ', 1)[0])\n",
    "operating_systems = user_agent[[\"os\"]].map(lambda x: x.rsplit(' ', 1)[0])\n",
    "user_agent = user_agent.assign(os=operating_systems, browser=browsers)\n",
    "user_agent"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "622da49f8515b39a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the data in a more manageable state, we can now calculate the distribution of traffic by browser and operating system:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "988457ba9481f7d7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "browsers_top = get_top_n(user_agent.browser, 5)\n",
    "operating_systems_top = get_top_n(user_agent.os, 5)\n",
    "\n",
    "figure(figsize=(20, 10))\n",
    "subplot(1, 2, 1)\n",
    "bar(browsers_top.index, browsers_top[\"count\"])\n",
    "subplot(1, 2, 2)\n",
    "bar(operating_systems_top.index, operating_systems_top[\"count\"])\n",
    "show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2feec4069fb17e59",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To no one's surprise, Chrome reigns supreme in the browser market, closely followed by Edge and Firefox. Although we develop and test our platform UI exclusively on Chrome, the fact that a third of the platform visits comes from other browsers puts an emphasis on the need for cross-browser compatibility, which would be ensured through end-to-end testing. While manual testing would be too time-consuming, automated testing would be a more efficient solution. Although you might be asking yourself why we are considering operating systems for a web application, the answer lies in the fact that the platform is also accessible through mobile devices. As such, we made sure that the design of the site was responsive. While the data shows that a minute fraction of the traffic comes from mobile devices, it's important to keep in mind that a good first impression from a handheld can convince users to try the desktop version as well."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ed7bd9054ba52d9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To conclude our analysis, let's take a look at what users are searching for, starting with the most popular query parameters.\n",
    "Note that for this analysis, we will not consider:\n",
    "- `page` and `per_page`, as they are used for pagination\n",
    "- `nameEquals`, as it is always included in the query\n",
    "Furthermore, the ranged parameters (e.g. `starsMin` and `starsMax`) will be grouped under a single umbrella (i.e. `stars`)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d5acbfb84ae4f8d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "blacklist = [\"page\", \"per_page\", \"nameEquals\"]\n",
    "query_parameters = df[\"query\"].map(lambda query: query.keys())\n",
    "query_parameters_by_query = query_parameters.explode()\n",
    "query_parameters_by_query = query_parameters_by_query.loc[~query_parameters_by_query.isin(blacklist)]\n",
    "query_parameters_by_query = query_parameters_by_query.str.removesuffix(\"Min\")\n",
    "query_parameters_by_query = query_parameters_by_query.str.removesuffix(\"Max\")\n",
    "query_parameters_by_query_count = query_parameters_by_query.value_counts()\n",
    "query_parameters_by_query_count.plot.bar(figsize=(20, 10))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca6302cba92b9e54",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To no particular surprise, we can see that `language` comes out on top, followed closely by `stars`, `name` and `commits`. What is interesting however is that the `topic` parameter placed in the top 10, in spite of being a relatively new addition to the dataset. This could be an indication that the feature is being well received by the users. On the flipside, exact match  parameters such as `label` and `license` have seen significantly less use. This is most likely due to the fact that there is currently no way to specify multiple values for a single query, which i my humble opinion is a feature we should consider adding in the future."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "628945bb14175a70"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given that the `language` parameter is the most popular, it would be interesting to see which languages users search for the most. To do this, we can extract the `language` parameter values and calculate their distribution:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ade8991bb286f5e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_languages(query: dict):\n",
    "  if \"language\" in query:\n",
    "    first, = query[\"language\"]\n",
    "    return first.lower()\n",
    "  return None\n",
    "\n",
    "query_parameter_language = df[\"query\"].map(get_languages).dropna()\n",
    "query_parameter_language_count = get_top_n(query_parameter_language, 4)\n",
    "figure(figsize=(15, 7))\n",
    "pie(query_parameter_language_count[\"count\"], labels=query_parameter_language_count.index, autopct=autopct)\n",
    "show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fff8f691337bf38",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As was expected, `Java`, `Python` and `JavaScript`, the three of the most popular programming languages in the world constitute a cumulative 65% of language-specific searches. While looking at the top does not reveal anything particularly surprising, it's at the very bottom where some new insights are obtained:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64ed523adf35d9c2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "least_popular_languages = query_parameter_language.value_counts().tail()\n",
    "for item in list(least_popular_languages.keys()):\n",
    "    print(item)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3fe48c6d53720806",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first thing that stands out is the fact that `Lua` and `Haskell` were both queried. Although we do not mine these languages, it's worth considering adding them to the list of future targets. The second thing that stands out is the fact that `dart, kotlin` and `c/c++` were search terms. Echoing the sentiment from the previous section, it's worth considering adding the ability to search for multiple languages at once, allowing sampling on a set of languages."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be81b482a1304a2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
